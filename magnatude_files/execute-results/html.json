{
  "hash": "58d932e47304d34fd3ba3489f433f6e0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Magnitude Matters: When Direction Is the Wrong Question\"\ntoc: true\nnumber-sections: true\ntoc-depth: 4\nreading-time: true\nexecute:\n  warning: false\n  message: false\nformat:\n  html:\n    code-fold: show\n    code-summary: \"Code\"\n---\n## When “no overall effect” hides the real signal\n\nIn the previous posts in this series, I have argued that two common responses to messy ecological evidence often fail us:\n\n- Counting studies instead of synthesising them  \n- Relying on pooled averages even when heterogeneity dominates  \n\nAt this point, a natural question arises:\n\n> *If direction is inconsistent and averages can be misleading, what should we be looking at instead?*\n\nOne answer *could* be **magnitude**.\n\nThis post is about why, in many ecological questions, *direction is contingent* but *magnitude is informative* and about what must change statistically before magnitude-based synthesis is defensible.\n\n## The cancellation problem\n\nConsider a simple and very common situation.\n\n- Some studies report strong positive effects  \n- Some report strong negative effects  \n- The pooled mean effect is close to zero  \n\nA standard meta-analysis might reasonably conclude:\n\n> “There is no overall effect.”\n\nBut in ecology, this conclusion can be deeply misleading.\n\nA system in which effects are sometimes strongly positive and sometimes strongly negative is not *unresponsive*. It is *highly responsive* just in context-dependent ways.\n\nThe problem is not the statistics.  \nThe problem is the question being asked.\n\n## A concrete ecological example: grazing and plant diversity\n\nConsider the effects of **grazing intensity on plant species richness** one of the most studied relationships in terrestrial ecology.\n\nAcross studies and systems, grazing has been shown to:\n\n- **Increase plant diversity** at low to moderate intensity  \n  (by reducing dominance and opening space for subordinates)\n- **Decrease plant diversity** at high intensity  \n  (through biomass removal, trampling, and soil degradation)\n\nNow imagine a meta-analysis that pools studies across:\n\n- Different livestock species  \n- Different productivity gradients  \n- Different baseline disturbance regimes  \n- Different management histories  \n\nSome studies report strong positive effects of grazing on richness.  \nOthers report strong negative effects.\n\nWhen combined, the mean effect size may sit very close to zero.\n\nA standard random-effects meta-analysis might conclude:\n\n> “Grazing has no overall effect on plant species richness.”\n\nStatistically, that statement may be correct. Ecologically, it is almost meaningless.\n\nA driver that can *strongly increase* or *strongly decrease* biodiversity depending on context is not benign. \n\nThe problem here is not heterogeneity.  \nThe problem is asking the wrong first question.\n\nBefore asking *whether grazing increases or decreases diversity on average*, a more fundamental question is:\n\n> *Does grazing produce large changes in community structure at all?*\n\nThat is a question about **magnitude**, not direction.\n\n## A simple simulation\n\nTo make this concrete, we simulate an evidence base where:\n\n- Effects are often large  \n- The sign of effects varies across studies  \n- Sampling error and heterogeneity are realistic  \n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Simulating bidirectional but strong effects\"}\nlibrary(tidyverse)\nlibrary(metafor)\n\nset.seed(4)\n\nsimulate_bidirectional <- function(\n  k = 30,\n  magnitude = 0.5,\n  tau = 0.3,\n  vi_range = c(0.05, 0.25)\n) {\n  vi <- runif(k, vi_range[1], vi_range[2])\n  signs <- sample(c(-1, 1), k, replace = TRUE)\n  theta <- signs * magnitude + rnorm(k, 0, tau)\n  yi <- rnorm(k, theta, sqrt(vi))\n\n  tibble(\n    study = seq_len(k),\n    yi = yi,\n    vi = vi,\n    sei = sqrt(vi)\n  )\n}\n\ndat <- simulate_bidirectional()\n```\n:::\n\n\n## What standard meta-analysis concludes\n\nWe first fit a conventional random-effects meta-analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Standard random-effects meta-analysis\"}\nm_signed <- rma(yi, vi, data = dat, method = \"REML\")\npredict(m_signed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n    pred     se   ci.lb  ci.ub   pi.lb  pi.ub \n -0.0104 0.1254 -0.2561 0.2353 -1.1299 1.1091 \n```\n\n\n:::\n:::\n\n\nWe get:\n\n- A pooled effect near zero  \n- A confidence interval that overlaps zero  \n- A conclusion of “no overall effect”  \n\nThis conclusion is statistically defensible but ecologically incomplete.\n\n## The temptation: synthesising absolute effects\n\nA natural response is to say:\n\n> *If direction cancels out, why not synthesise the size of effects instead?*\n\nThis leads to the idea of magnitude-based synthesis, often operationalised by taking absolute values of effect sizes.\n\nHowever, this step requires care.\n\nNaïvely meta-analysing absolute effect sizes **violates the assumptions of standard meta-analysis** and will generally produce biased results if done without adjustment.\n\n## Why naïve magnitude meta-analysis is invalid\n\nTransforming effect sizes to absolute values breaks several core assumptions at once:\n\n1. **Systematic upward bias**  \n   Even when true effects are zero, the expected absolute value of noisy estimates is positive. Noise becomes signal.\n\n2. **Non-normal sampling distributions**  \n   Absolute values follow a folded distribution that is skewed and asymmetric, not approximately normal.\n\n3. **Invalid variances**  \n   The original sampling variances no longer describe uncertainty in the transformed effects, leading to incorrect weighting.\n\nPut simply:\n\n> *A naïve meta-analysis of absolute effect sizes will almost always suggest an effect, even when none exists.*\n\n## What must change for magnitude-based synthesis to be defensible\n\nIf magnitude is the question, then the **estimand and model must change**. At least one of the following adjustments is required.\n\n### Option 1: Bias-corrected magnitudes \n\nFor normally distributed sampling error, the expected absolute value under a true null effect is:\n\n$$\nE(|y_i| \\mid \\theta = 0) = \\sqrt{\\frac{2 v_i}{\\pi}}\n$$\n\nA simple bias-corrected magnitude is therefore:\n\n$$\n|y_i| - \\sqrt{\\frac{2 v_i}{\\pi}}\n$$\n\nThis removes the expected contribution of sampling noise and ensures that zero effects map approximately to zero.\n\nThis approach is imperfect, but vastly preferable to naïve absolute-value synthesis.\n\n### Option 2: Explicit modelling of magnitude (folded-normal)\n\nA cleaner solution is to model magnitude with an appropriate likelihood that acknowledges the transformation.\n\nConceptually:\n\n- Signed effects are generated with normal error  \n- Magnitudes follow a folded distribution  \n- The target of inference is the distribution of *true* effect magnitudes  \n\nThis approach avoids violated assumptions and makes uncertainty explicit.\n\nBelow is a minimal working example using `rstan`, treating the sampling standard errors as known.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Folded-normal magnitude model (rstan)\"}\nlibrary(rstan)\nlibrary(posterior)\n\nrstan_options(auto_write = TRUE)\noptions(mc.cores = parallel::detectCores())\n\nabs_yi <- abs(dat$yi)\nsei    <- dat$sei\n\nstan_code <- \"\nfunctions {\n  real folded_normal_lpdf(real y, real mu, real sigma) {\n    // y >= 0; folded normal = mixture of N(+mu,sigma) and N(-mu,sigma)\n    return log_sum_exp(\n      normal_lpdf(y |  mu, sigma),\n      normal_lpdf(y | -mu, sigma)\n    );\n  }\n}\ndata {\n  int<lower=1> N;\n  vector<lower=0>[N] y;       // observed magnitudes |yi|\n  vector<lower=0>[N] se;      // known sampling SDs\n}\nparameters {\n  real<lower=0> mu_mag;       // population mean magnitude\n  real<lower=0> tau_mag;      // heterogeneity (SD) among true magnitudes\n  vector<lower=0>[N] m;       // true study magnitudes\n}\nmodel {\n  // Priors (tweak as needed for your effect-size scale)\n  mu_mag ~ normal(0, 1);\n  tau_mag ~ exponential(2);\n\n  // Hierarchical distribution of true magnitudes\n  m ~ normal(mu_mag, tau_mag);\n\n  // Observation model: folded normal\n  for (n in 1:N) {\n    target += folded_normal_lpdf(y[n] | m[n], se[n]);\n  }\n}\ngenerated quantities {\n  // Posterior predictive: a new study's true magnitude\n  real m_new = fabs(normal_rng(mu_mag, tau_mag));\n}\n\"\n\nstan_data <- list(\n  N  = nrow(dat),\n  y  = abs_yi,\n  se = sei\n)\n\nfit <- stan(\n  model_code = stan_code,\n  data = stan_data,\n  chains = 4,\n  iter = 2000,\n  warmup = 1000,\n  seed = 1\n)\n\nprint(fit, pars = c(\"mu_mag\", \"tau_mag\", \"m_new\"), probs = c(0.025, 0.5, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n        mean se_mean   sd 2.5%  50% 97.5% n_eff Rhat\nmu_mag  0.55    0.00 0.07 0.41 0.55  0.71   394  1.0\ntau_mag 0.14    0.01 0.08 0.02 0.14  0.30    28  1.1\nm_new   0.55    0.00 0.17 0.19 0.55  0.91  1613  1.0\n\nSamples were drawn using NUTS(diag_e) at Thu Feb  5 12:25:40 2026.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n```\n\n\n:::\n:::\n\n\nTo summarise the posterior more neatly:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Posterior summaries for key magnitude parameters\"}\ndraws <- as_draws_df(fit, pars = c(\"mu_mag\", \"tau_mag\", \"m_new\"))\n\nsummary_df <- tibble(\n  param = c(\"mu_mag\", \"tau_mag\", \"m_new\"),\n  mean  = c(mean(draws$mu_mag), mean(draws$tau_mag), mean(draws$m_new)),\n  q025  = c(quantile(draws$mu_mag, 0.025), quantile(draws$tau_mag, 0.025), quantile(draws$m_new, 0.025)),\n  q50   = c(quantile(draws$mu_mag, 0.5),   quantile(draws$tau_mag, 0.5),   quantile(draws$m_new, 0.5)),\n  q975  = c(quantile(draws$mu_mag, 0.975), quantile(draws$tau_mag, 0.975), quantile(draws$m_new, 0.975))\n)\n\nsummary_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  param    mean   q025   q50  q975\n  <chr>   <dbl>  <dbl> <dbl> <dbl>\n1 mu_mag  0.555 0.408  0.553 0.708\n2 tau_mag 0.142 0.0228 0.135 0.302\n3 m_new   0.553 0.189  0.552 0.912\n```\n\n\n:::\n:::\n\n\n\n## A visual comparison: naïve |yi| meta-analysis vs folded-normal magnitude model\n\nTo make the difference concrete, we can compare:\n\n- A *naïve* random-effects meta-analysis of `abs(yi)` using `vi` as if nothing changed (this is not valid!)\n- The posterior distribution for `mu_mag` from the folded-normal model\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Plot: naive magnitude meta-analysis vs folded-normal posterior\"}\nlibrary(ggplot2)\n\n# Naïve magnitude meta-analysis (illustrative only; violates assumptions)\nm_naive_abs <- rma(abs(dat$yi), dat$vi, method = \"REML\")\nnaive_est <- as.numeric(m_naive_abs$b)\nnaive_ci  <- c(as.numeric(m_naive_abs$ci.lb), as.numeric(m_naive_abs$ci.ub))\n\n# Posterior for mean true magnitude from folded-normal model\nmu_draws <- draws$mu_mag\nmu_ci <- quantile(mu_draws, probs = c(0.025, 0.5, 0.975))\n\n# Density data for plotting\ndens <- density(mu_draws)\ndens_df <- tibble(x = dens$x, y = dens$y)\n\n# Build the plot\nggplot(dens_df, aes(x = x, y = y)) +\n  geom_line(linewidth = 1) +\n  labs(\n    x = \"Mean true magnitude (mu_mag)\",\n    y = \"Posterior density\",\n    title = \"Magnitude inference: folded-normal model vs naive |yi| meta-analysis\",\n    subtitle = \"Naive approach treats |yi| as Gaussian with variance vi (illustrative only)\"\n  ) +\n  # Posterior median and 95% credible interval\n  geom_vline(xintercept = mu_ci[2], linetype = \"solid\", linewidth = 0.8) +\n  geom_vline(xintercept = mu_ci[c(1, 3)], linetype = \"dashed\", linewidth = 0.8) +\n  # Naive point estimate and CI\n  geom_vline(xintercept = naive_est, linetype = \"dotdash\", linewidth = 0.8) +\n  geom_vline(xintercept = naive_ci, linetype = \"dotted\", linewidth = 0.8) +\n  annotate(\"text\", x = mu_ci[2], y = max(dens$y), vjust = -0.5,\n           label = \"Folded-normal posterior median\", size = 3.2) +\n  annotate(\"text\", x = naive_est, y = max(dens$y)*0.85, vjust = -0.5,\n           label = \"Naive |yi| estimate\", size = 3.2) +\n  coord_cartesian(ylim = c(0, max(dens$y)*1.1))\n```\n\n::: {.cell-output-display}\n![](magnatude_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nIn this figure:\n\n- The **solid line** is the posterior density for the mean true magnitude (`mu_mag`) from the folded-normal model.\n- The **solid vertical line** is the posterior median; **dashed** lines show the 95% credible interval.\n- The **dot-dash** vertical line is the naïve random-effects estimate from meta-analysing `abs(yi)` directly; **dotted** lines show its 95% CI.\n\nThe point is not that the naïve approach is always “wildly wrong” in every dataset. The point is that it is answering the question using an invalid sampling model and when sampling variances differ, that can materially change inference.\n\n### Option 3: Shift the estimand entirely\n\nIn many cases, it is more informative to estimate quantities such as:\n\n- The probability that effects exceed a meaningful threshold  \n- The median or upper quantiles of effect magnitude  \n- The proportion of contexts with large responses  \n\nThese quantities align naturally with decision-making and avoid cancellation without relying on problematic averages.\n\n## What magnitude-based synthesis is (and isn’t)\n\nMagnitude-based synthesis is **not**:\n\n- Ignoring direction permanently  \n- Claiming all effects are beneficial or harmful  \n- A shortcut around careful modelling  \n\nIt **is**:\n\n- A way to detect responsiveness  \n- A guard against misleading cancellation  \n- A complement to directional and context-specific analyses  \n\nUsed appropriately, it helps answer the question:\n\n> *Does this driver produce changes large enough to matter?*\n\n## When magnitude is the right question\n\nMagnitude-based synthesis is particularly informative when:\n\n- Effects are expected to be bidirectional  \n- Context dependence is strong  \n- Management decisions hinge on *risk*, not mean response  \n\nCommon ecological examples include:\n\n- Disturbance regimes  \n- Climate variability  \n- Light, nutrients, or grazing intensity  \n- Invasive species impacts  \n\nIn these cases, “no average effect” is often the wrong conclusion.\n\n## From magnitude to context\n\nMagnitude tells us *whether* systems respond.  \nContext tells us *how* they respond.\n\nOnce we know that effects are large, the most important question becomes:\n\n> *Under what conditions do outcomes differ?*\n\nAnswering that requires treating heterogeneity as signal, not noise.\n\n## Closing thought\n\nA synthesis that averages away large but opposing effects is not neutral it is blind to the very dynamics ecologists often care about most.\n\nMagnitude-based synthesis can reveal those dynamics but only if the statistical model is aligned with the question being asked.\n\n",
    "supporting": [
      "magnatude_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}